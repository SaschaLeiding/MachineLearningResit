---
title: "MachineLearningResit"
author: '12393007'
date: "`r Sys.Date()`"
output: word_document
---

# Strategy and To-Dos

Prior need to analyse ALLSPEECHES for TYPE OF POLITICIAN Done (i) .number_speeches = How Vocal is politician Done (ii) .speechlength = avg. length of speech (iii) generality = how many basic terms per speech (iv) content = TOPIC MODELLING by various indizes -\> may need to transform numerical corrindex and income into categorical


# Install & Load Packages
```{r packages}
  #install.packages("tidyverse")
  #install.packages("tidytext")
  #install.packages("tidymodels")
  #install.packages("glmnet")
  #install.packages("glmnetUtils")
  #install.packages("wordcloud")
  #install.packages("patchwork")
  #install.packages("hdm")
  #install.packages("grf")
  #install.packages("topicmodels")
  #install.packages("SnowballC")

  library(tidyverse)
  library(tidytext)
  library(tidymodels)
  library(glmnet)
  library(glmnetUtils)
  library(wordcloud)
  library(patchwork)
  library(hdm)
  library(grf)
  library(topicmodels)
  library(SnowballC)

```

# Load Data
```{r LoadData}
  load("./Data/politicians.rdata")
```

# Mutate and Tidy data
```{r MutateTidy}
# Split column 'allspeeches' into tokens in column 'word', flattening the table into one-token-per-row
  data_unnested <- politicians %>%
    unnest_tokens(.word, allspeeches) %>%
    select(speaker, .word) %>%
    rename(.speaker = speaker)
  
  new_colnames <- paste0(".", colnames(politicians))
  colnames(politicians) <- new_colnames
  
  data <- politicians %>% 
    mutate(.corrupt = ifelse(.corrindex > 0, 1, 0), # Dummy whether more or less corrupt than the avg. politician
           .number_speeches = str_count(.allspeeches, "\\t House of Commons Hansard Debates for ") + 1, # create variable with number of speeches per speaker in variable 'allspeeches'
           .number_words = str_count(.allspeeches, " ") + 1, # Count total number of words
           .speechlength = .number_words/.number_speeches, # calc. avg. length of speech
           .partydummy = ifelse(.party == "B", 1, 0), # Transform to dummy for processing power
           .logincome = log(.income)) %>% # Transform income into log
    group_by(.birthplace) %>%
    # ASSUMPTION: all politicians within a birthplace are included in the data, or distribution and choice of entries are representative for a region
    mutate(.regionideology = sum(.party == 'A') - sum(.party == 'B'), # Identify Ideological tendency for each region
           .regionintensity = 1 - sum(.party == ifelse(sum(.party == 'A') > sum(.party == 'B'), 'A', 'B')) / n())  %>% # Identify the more prevalent party and calculate the ratio
    ungroup()
  
  # Transform Categorical variables to dummies, as transformation to as.factor() needs to much processing
  for (i in 1:21) {
      new_col <- paste0(".birthplace", i)
      data[new_col] <- ifelse(data$.birthplace==i, 1, 0)
    }
  
  apply(is.na(data), 2, which)
```

# Data Overview
Get quick overview of data - Summary of Data, Plotting main variables (corrindex, income, birthplace by Party) To-Do: Select output for paper and delete irrelevant memory things
```{r Overview}
summary(data)
  "
  - age between 35 and 70 with mean of 47.33
  - 23 birthplaces
  - corruption index between -2.633 and 3.292 with mean of 0
  - income between 52,231 and 532,325 with mean of 112,303
  "
  
  # Plot corrindex against income by Party association
  {
    plot_outcome <- ggplot(data = data, 
                           aes(x = .logincome, y = .corrindex, color = .party, group = .party)) +
      geom_point()
    print(plot_outcome)
    
    "
  Plot indicates that for both parties there is a positive correlation between
  corrupt behavior and income, thus more corrupt behavior indicates higher income.
  Slightly indicates that individuals associated to party B have less corrupt
  behavior
  "
  }
  
  # Plot Birthplace and Party Affiliation
  {
    plot_birthplace <- ggplot(data = data, aes(x = .party, fill = .party)) +
      geom_bar(position = "dodge") +
      facet_wrap(~.birthplace, scales = "free_x", ncol = 3) +
      labs(title = "Speakers by Party and Birthplace",
           x = "Party",
           y = "Number of Speakers") +
      theme_minimal()
    
    print(plot_birthplace)
  }
  
  # Density Plot of Income and Corruption and  by Party
  {
    plot_density_income <- ggplot(data = data,
                                  aes(x = .logincome, color = .party, group = .party)) +
      geom_density(linewidth = 1) +
      labs(x = "Income",
           y = "Density")
    print(plot_density_income)
    
    
    plot_density_corr <- ggplot(data = data,
                                aes(x = .corrindex, color = .party, group = .party)) +
      geom_density(linewidth = 1) +
      labs(x = "Corruption Index",
           y = "Density")
    print(plot_density_corr)
  }
```

# Sentiment
```{r sentiment}
# Calculate sentiment
{
  "
  when using dictionary 'bing no need to filter out the stem as it contains all
  word alterations
  "
  sentiment_dict <- get_sentiments("bing")
  sentiment <- data_unnested %>% 
    inner_join(sentiment_dict, relationship = "many-to-many", join_by(.word == word)) %>% # include only words that are part of the defined sentiment dictionary
    count(.speaker, sentiment) %>% # Counts for each speaker and both sentiments the number of words associated to each sentiment
    pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% #transforms table so each row is a speaker again
    # Mutate Command works only for 'bing' dictionary 
    mutate(.sentiment = positive - negative) %>% # calculate overall sentiment index
    rename(.positive = positive,
           .negative = negative)
  
  data <- data %>% left_join(sentiment, join_by(.speaker == .speaker))
}

# Plot sentiment on Log Income and Corruption Index
{
  plot_sentiment_inc <- ggplot(data = data,
                               aes(x=.sentiment, y = .logincome, color = .party, group = .party)) +
    geom_point() +
    geom_smooth(method='lm', formula=y~x, se=FALSE)
  print(plot_sentiment_inc)
  
  plot_sentiment_corr <- ggplot(data = data,
                                aes(x=.sentiment, y = .corrindex, color = .party, group = .party)) +
    geom_point() +
    geom_smooth(method='lm', formula=y~x, se=FALSE)
  print(plot_sentiment_corr)
}

rm(sentiment)
rm(sentiment_dict)
```

# Linear Estimation of constructed variables and Sentiment
```{r linestimation}
# Create string with names of independent variables, drop ID, dependent or double variables/derived variables
    independent_var <- colnames(data %>% 
                                  select(!c(".speaker", # ID variables
                                            ".corrindex", ".corrupt", ".income",".logincome", # Dependent
                                            ".party", ".allspeeches", ".birthplace", ".sentiment"))) # Derived variables
    
    # Create linear regression models for corruption index and income
    basemodel_corr <- lm((reformulate(independent_var, response =".corrindex")), data = data)
    basemodel_income <- lm((reformulate(independent_var, response =".logincome")), data = data)
    
    # Pritn out results of simple regression models
    summary(basemodel_corr) # If party=1, thus B, .corrindex drops by 0.1989 at 0.001% significance level
    summary(basemodel_income)
```

# Prepare Speech Analysis
```{r prepspeechanalysis}
  # Set a seed to reconstruct analyses
  set.seed(12345)
  # Define variables
  y_var <- '.corrindex' # Choose between '.corrindex' or '.logincome'
  word_type <- '.word' # Choose between '.word' or '.stem'

  # pick the words to keep as predictors
  # Need to choose between taking full words or word stems only (words = 16.6k vs. stem = 12k entries)
  {
    words_to_keep <- unique(data_unnested %>% # Drop double entries (drops words from 484k to 16,603) with smart to 16,280 and .stem dropping to 12,030
                              anti_join(get_stopwords(source="smart"), # standard=175 words vs, smart 571 words
                                        join_by(.word  == word)) %>% # Drop words that are in dictionary stopwords, e.g.: I , me, my, myself...
                              mutate(.wordtype = if(word_type == '.word'){.word}
                                     else{wordStem(.word)}) %>%
                              count(.wordtype) %>%
                              
                              # is first filter necessary?
                              filter(str_detect(.wordtype, '.co|.com|.net|.edu|.gov|http', negate = TRUE)) |> # return all entries in 'word' that do NOT contain the listed words of URLs
                              filter(str_detect(.wordtype, '[0-9]', negate = TRUE)) |> # return all entries in 'words' that contain no numbers
                              
                              # How do I justify the value of 2?
                              filter(n > 2) |> # take only words that occur more than two times
                              pull(.wordtype)) # extract the column word as vector
  }
  
  # Construct Term Frequencies
  {
    tidy_speech <- data_unnested %>%
      transmute(.wordtype = if(word_type == '.word'){.word}
                else{wordStem(.word)}) %>%
      rename(.word = .wordtype) %>%
      filter(.word %in% words_to_keep) %>% # Take only words that are in previous created list
      count(.speaker, .word) %>% # count per speaker, identified through '.word' the number of a word occuring
      # Bind the term frequency and inverse document frequency of the data to the dataset
      bind_tf_idf(term = '.word', # Column containing terms as string or symbol
                  document = '.speaker', # Column containing document IDs as string or symbol
                  n = 'n') %>% # Column containing document-term counts as string or symbol
      select(.speaker, .word, tf) %>% # select ID, word and term-frequency column
      
      filter(.word != "") %>% # Filter out EMPTY strings
      # pivot wider into a document-term matrix
      pivot_wider(id_cols = '.speaker',
                  names_from = '.word',
                  values_from = 'tf',
                  values_fill = 0)
  }
  
  # Join Term Frequency with dataset
  {
    tidy_speech <- data %>%
      select(!c(".allspeeches")) %>%
      right_join(tidy_speech, by = '.speaker') %>%
      ungroup()
  }
  
  # Create train and test sample from data merge with Term Frequency
  # Too little data for stratification by BIRTHPLACE
  {
    # split sample into training and test sample
    data_split <- initial_split(tidy_speech, prop = 0.8) # split into 80% training and 20% test
    
    train <- training(data_split)
    test <- testing(data_split)
  }
  
  # Remove dataset and clear memory for computation
  {
    rm(data_split)
    #rm(data_unnested)
    #rm(words_to_keep)
    gc()
  }
```

# Conduct Speech Analysis
```{r speechanalysis}
# Set a seed to reconstruct analyses
set.seed(12345)
#y_var <- '.corrindex' # Choose between '.corrindex' or '.logincome'
# y_var must be same in code-chunk 'prepspeechanalysis'

# Change GGplot setting
theme_update(plot.subtitle = element_text(hjust = 0.5))

# Define Variables
{
  confounder_names <- c(".speaker", # Identifier
                        ".corrindex", ".corrupt", ".income", ".logincome", # Outcome Variables
                        ".party", ".partydummy", # Target Parameter
                        grep("\\.birthplace", colnames(train), value =TRUE)) # Instrument
  # Train Data
  Y_train <- train[y_var] # Outcome Variable
  W_train <- train[, !(colnames(train) %in% confounder_names)]# Confounders - Nuisance Parameters
  D_train <- train['.partydummy'] # Target Parameter
  Z_train <- train %>% select(starts_with(".birthplace") & !(".birthplace")) # Instrument
  X_train <- cbind(D_train, Z_train, W_train) # Observed Regressors - 16,635 variables

  # Test Data
  Y_test <- test[y_var] # Outcome Variable
  W_test <- test[, !(colnames(test) %in% confounder_names)] # Covariates - Nuisance Parameters
  D_test <- test['.partydummy'] # Target Parameter
  Z_test <- test %>% select(starts_with(".birthplace") & !(".birthplace")) # Instrument
  X_test <- cbind(D_test, Z_test, W_test) # Observed Regressors - 16,635 variables
  
  # Full Data
  Y_full <- tidy_speech[y_var] # Outcome Variable
  W_full <- tidy_speech[, !(colnames(tidy_speech) %in% confounder_names)] # Covariates - Nuisance Parameters
  D_full <- tidy_speech['.partydummy'] # Target Parameter
  Z_full <- tidy_speech %>% select(starts_with(".birthplace") & !(".birthplace")) # Instrument
  X_full <- cbind(D_full, Z_full, W_full) # Observed Regressors - 16,635 variables
  
  # Other
  simplemodels_list <- list()
  in_sample_plots <- list()
  out_sample_plots <- list()
  name_outcome <- ifelse(y_var == ".corrindex", "corr", "inc")
}

# 2.1 Identifying important Words/variables to control for type of politician
{
  # (1) Simple Model - Linear Regression with all words
  simplemodels_list[[paste0("model_comp_", name_outcome)]]<- linear_reg() %>%
    fit_xy(y = Y_train, x = X_train)

  # (2) Regularized Model - LASSO
  simplemodels_list[[paste0("model_regul_", name_outcome)]]<- linear_reg(penalty = 0.01, # Amount of Regularization (lambda)
                                                                         mixture = 1) %>% # Pure LASSO model
    set_engine('glmnet') %>%
    fit_xy(y = Y_train, x = X_train)

  # (3) Regularized Model - LASSO - lambda Cross-validated (CV)
  simplemodels_list[[paste0("model_cvregul_", name_outcome)]]<- cv.glmnet(x = as.matrix(X_train), 
                                                    y = as.matrix(Y_train),
                                                    nfolds = 10, 
                                                    type.measure = "mse")
  
  # (4) Regression forest for Propensity
  ZW_full <- cbind(Z_full, W_full)
  propensity.forest <- regression_forest(X= ZW_full,
                                         Y= as.matrix(D_full))
  
  # Predict probability of treatment assignment for each value:
  propensity.hat <- predict(propensity.forest)
  propensity.importance <- variable_importance(propensity.forest)
  propensity.importance.cols <- order(propensity.importance, decreasing = TRUE)
  colnames(ZW_full)[propensity.importance.cols][1:24] # Print out 10 most imp. variables
  
  plot_hist_propensity <- hist(propensity.hat[, "predictions"], 
                               main = "Estimate of propensity score distribution", 
                               xlab = "Propensity score")
  ZW_full$p.hat <- propensity.hat$predictions
} 

# 2.1 Plotting of in-/out-of-sample fit for models (1)-(3)
{
  # Loop through each model in simplemodels_list
  for (i in seq_along(simplemodels_list)) {
    model <- simplemodels_list[[i]]
    model_name <- names(simplemodels_list)[i]

    # Check if the model is a cv.glmnet model
    if (inherits(model, "cv.glmnet")) {
      # For cv.glmnet, use lambda.min
      predict_in_sample <- predict(model, newx = as.matrix(X_train), s = "lambda.min", type = "link")
      predict_out_sample <- predict(model, newx = as.matrix(X_test), s = "lambda.min", type = "link")

      
      # Plotting for cv.glmnet
      plot_in_sample <- ggplot(data = (train %>% bind_cols(predict_in_sample)),
                               aes(x = lambda.min, y = .corrindex)) +
        geom_point() +
        geom_abline(intercept = 0, slope = 1, size = 0.5) +
        labs(x = "Predicted Corrindex", y = "Actual Corrindex")
      
      plot_out_sample <- ggplot(data = (test %>% bind_cols(predict_out_sample)),
                              aes(x = lambda.min, y = .corrindex)) +  
        geom_point() +
        geom_abline(intercept = 0, slope = 1, size = 0.5) +
        labs(x = "Predicted Corrindex", y = "Actual Corrindex")
      } else {
        # For other models
        predict_in_sample <- predict(model, X_train)
        predict_out_sample <- predict(model, new_data = X_test)
        
        # Common code for plotting fits
        plot_in_sample <- ggplot(data = (train %>% bind_cols(predict_in_sample)),
                                 aes(x = .pred, y = .corrindex)) +
          geom_point() +
          geom_abline(intercept = 0, slope = 1, size = 0.5) +
          labs(x = "Predicted Corrindex", y = "Actual Corrindex")
        
        plot_out_sample <- ggplot(data = (test %>% bind_cols(predict_out_sample)),
                                  aes(x = .pred, y = .corrindex)) +
          geom_point() +
          geom_abline(intercept = 0, slope = 1, size = 0.5) +
          labs(x = "Predicted Corrindex", y = "Actual Corrindex")
        }
    
    # Store combined in-sample and out-of-sample plots
    in_sample_plots[[model_name]] <- plot_in_sample
    out_sample_plots[[model_name]] <- plot_out_sample
    }
  
  combined_plots <- (Reduce('+', in_sample_plots) / Reduce('+', out_sample_plots)) +
    plot_layout(guides = 'collect')
  
  label_plots <- map((c("OLS", "LASSO", "LASSO CV")), ~ ggplot() +
                       theme_void() +
                       theme(plot.margin = margin(0, 0, 0, 0)) +
                       annotate("text", x = 0.5, y = 0.5, label = .x, fontface = "bold", hjust = 0.5))
  
  final_plot <- ((label_plots[[1]] | label_plots[[2]] | label_plots[[3]]) / combined_plots) +
    plot_layout(heights = c(1, 10))
  print(final_plot)
}

# 2.2 Estimate Effect of Party Affiliation on .corrindex and .logincome
{
  # Double LASSO - CV
  {
    # Partial out W (confounders) from Y (outcome)
    fit.lasso.Y <- cv.glmnet(x=as.matrix(W_full), y=as.matrix(Y_full))
    fitted.lasso.Y <- predict(fit.lasso.Y, newx=as.matrix(W_full), s="lambda.min")
    Ytilde_corr <- as.matrix(Y_full) - fitted.lasso.Y
    
    # Partial out w(confounders) from D(Party Affiliation)
    fit.lasso.D <- cv.glmnet(x=as.matrix(W_full), y=as.matrix(D_full))
    fitted.lasso.D <- predict(fit.lasso.D, newx=as.matrix(W_full), s="lambda.min")
    Dtilde_corr <- as.matrix(D_train) - fitted.lasso.D
    
    # Run OLS of Ytilde on Dtilde
    fit.doubleLASSO <- lm(Ytilde_corr ~ Dtilde_corr)
    beta1hat <- coef(fit.doubleLASSO)[2]
    print(beta1hat)
  }
  
  # Causal Forest
  {
    fit.forest <- causal_forest(as.matrix(W_full), as.matrix(Y_full), 
                                 as.matrix(D_full), tune.parameters = "all")
    
    # Calculate the ATE:
    average_treatment_effect(fit.forest)
    
    # The function best_linear_projection returns estimates of the linear relation between
    # the (conditional) ATE and the covariates. While we have no reason to assume that
    # the relation is linear, it presents a good first step to investigate if heterogeneity
    # is present and which variables appear to drive it.
    blp <- best_linear_projection(fit.forest, as.matrix(W_full))
    blp
  }
}


# 3 Estimation of Heterogeneity
{
  # Extract non-zero coefficients from Regularizing models
  {
    words_hetero <- c()
    # (2) LASSO - lambda = 0.1 - retrieves 923 variables
    words_hetero <- append(words_hetero,
                           tidy(simplemodels_list[[paste0("model_regul_", name_outcome)]]) %>%
                             filter(estimate != 0 & !grepl("^\\.", term)) %>% 
                             pull(term))
    
    # (3) LASSO - lambda = CV - retrieves 139 variables
    words_hetero <- append(words_hetero,
                           as.data.frame(as.matrix(coef(simplemodels_list[[paste0("model_cvregul_", name_outcome)]], 
                                  s = "lambda.min"))) %>% 
                             tibble::rownames_to_column(var = "term") %>%
                             filter(s1 != 0 & !grepl("^\\.", term)) %>%
                             pull(term))

    # (4) Double LASSO --- fit.lasso.D; fit.lasso.Y
    words_hetero <- append(words_hetero,
                           as.data.frame(as.matrix(coef(fit.lasso.D, s = "lambda.min"))) %>%
                             tibble::rownames_to_column(var = "term") %>%
                             filter(s1 != 0 & !grepl("^\\.", term)) %>%
                             pull(term))
    words_hetero <- append(words_hetero,
                           as.data.frame(as.matrix(coef(fit.lasso.Y, s = "lambda.min"))) %>%
                             tibble::rownames_to_column(var = "term") %>%
                             filter(s1 != 0 & !grepl("^\\.", term)) %>%
                             pull(term))

    # (5) Causal Forest
    words_hetero <- append(words_hetero,
                           data.frame(term = colnames(W_train),
                                      importance = variable_importance(forest = test.forest,
                                                                       max.depth = 1)) %>%
      filter(importance != 0 & !grepl("^\\.", term)) %>%
        pull(term))
    
    words_hetero <- setdiff(unique(words_hetero), "(Intercept)")
  }
   
  # Create  Interaction terms of .partydummy with all ppx
  pppx <- X_train %>% select(starts_with("."), all_of(words_hetero))
  pppx_interacted <- pppx %>% 
    mutate(across(everything(), ~ . * .partydummy, .names = ".partydummyx{.col}")) %>%
    select(where(~ var(.) != 0)) # drop all columns where there is no variation

  pppt <- rlassoEffects(x = (X_train %>% 
                               select(starts_with("."), all_of(words_hetero))),
                         y = Y_train)
  
}
```

# 4 Instrumental Variable
```{r Instrumental Variable}
# Normal IV (OLS does not work because too many variables due to all words contained), hence no residuals

#Quadruple LASSO IV -because instrument is multinomial
{
  set.seed(12345)
  # 1. Use LASSO to predict D with Z AND W
  fit.lasso.Dpred <- cv.glmnet(y=as.matrix(D_train), as.matrix((cbind(Z_train, W_train))))
  fitted.lasso.Dpred <- predict(fit.lasso.Dpred, newx=as.matrix((cbind(Z_train, W_train))), s="lambda.min")
  
  # 2. Use LASSO to partial out W
  # (a) partial out W from Y - done above
  #fit.lasso.Y <- cv.glmnet(as.matrix(W_train), as.matrix(Y_train))
  #fitted.lasso.Y <- predict(fit.lasso.Y, newx=as.matrix(W_train), s="lambda.min")
  #Ytilde <- as.matrix(Y_train) - fitted.lasso.Y
  
  # (b) partial out W from D - done above
  #fit.lasso.D <- cv.glmnet(as.matrix(W_train), as.matrix(D_train))
  #fitted.lasso.D <- predict(fit.lasso.D, newx=as.matrix(W_train), s="lambda.min")
  #Dtilde <- as.matrix(D_train) - fitted.lasso.D
  
  # (c) Partial out W from D_pred from 1.
  fit.lasso.QDpred <- cv.glmnet(as.matrix(W_train), fitted.lasso.Dpred)
  fitted.lasso.QDpred <- predict(fit.lasso.QDpred, newx=as.matrix(W_train), s="lambda.min")
  QDpredtilde <- fitted.lasso.Dpred - fitted.lasso.QDpred
  
  # 3. Estimate IV coefficient
  fit.QDLASSO.rf <- lm(Ytilde ~ QDpredtilde)
  coef.QDLASSO.rf <- coef(fit.QDLASSO.rf)[2]
  fit.QDLASSO.fs <- lm(Dtilde ~ QDpredtilde)
  coef.QDLASSO.fs <- coef(fit.QDLASSO.fs)[2]		# first-stage coefficient
  
  beta1hat.QDLASSO <- coef.QDLASSO.rf / coef.QDLASSO.fs	# IV coefficient
  beta1hat.QDLASSO

  
}
```


# Topic Modeling
```{r topicmodel}
save(topic_model, file = "topic_model.RData")
save(topic_model_22, file = "topic_model_22.RData")

load("./topic_model.RData")

special_list <- c("hon", "government", "people", "mr", "mrs", "one", "can", 
                  "also","member", "members", "house", "commitee")
speech_dtm <- data_unnested %>%
  transmute(.wordtype = if(word_type == '.word'){.word}
            else{wordStem(.word)}) %>%
  rename(.word = .wordtype) %>%
  filter(.word %in% words_to_keep) %>% # Take only words that are in previous created list
  count(.speaker, .word) %>%
  filter(n > 2 & nchar(.word) > 2 & !(.word %in% special_list)) %>% # words that occurred more than two times
  #filter(nchar(.word) > 2) %>% # words that have more than two characters
  cast_dtm(.speaker, .word, n)

topic_model <- LDA(speech_dtm, k=10, control=list(seed = 12345))

# obtain the topic-specific probability vectors theta_1,...,theta_K
# (in the LDA output, these are denoted by "beta")
speech_topics <- tidy(topic_model, matrix = "beta")

# find top 10 terms in each topic
speech_top_terms <- speech_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

# plot topic-specific probabilities of the top words in each topic
speech_top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()





topicsprob10 <- as.data.frame(posterior(topic_model)$topics)# Extract topic probabilities
colnames(topicsprob10) <- paste0(".topic", 1:ncol(topicsprob10)) # Rename columns
tidy_speech_tt <- cbin(tidy_speech, topicsprob10)
```


