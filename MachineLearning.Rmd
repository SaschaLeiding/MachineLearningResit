---
title: "MachineLearningResit"
author: '12393007'
date: "`r Sys.Date()`"
output: word_document
---

# Task and Variable Description

Variables: speaker = identifying number of the politician party = party affiliation of the politician; there are only two parties ("A" and "B") age0 = age of the politician when giving the first speech in parliament birthplace = identifying number of the region in which the politician was born married = indicator for marriage (1=married, 0=not married) corrindex = corruption index (larger values indicate larger probability of being corrupt) income = average annual income of the politician (average over entire working life) allspeeches = string containing all speeches the politician has given in parliament

Outcome Variables: 1. corrindex contains an index produced by an independent non-governmental institution (NGO), which is is supposed to measure how corrupt a politician is. The index has mean equal to zero, so positive (negative) values indicate that a politician is more (less) corrupt than the average politician. More precisely, the NGO argues that, under some assumptions, larger values of the index indicate a politician who is more likely to engage in corrupt behavior at some point during their political career.

2.  The variable income measures average annual income of the politician from age 40 to age 50.

Task: - To which extent does PARTY AFFILIATIOn affect CORRUPTION and INCOME -\> causal effect of PARTY AFFILIATION -\> Heterogeneity of causal effect (what TYPE of politician benefits more/less from PARTY AFFILIATION)

-   Explore two possibilities for establishing causality

1.  Use ALLSPEECHES to control for the TYPE of politician -\> text may contain a signal about what kind of politician they are -\> may proxy for confounding variables (those that simultaneously affect PARTY AFFILITION and the Outcome)

2.  Use BIRTHPLACE as an IV for PARTY AFFILIATION -\> speeches may not perfectly measure all confounders -\> endogeneity even conditional on the type

# Strategy and To-Dos

In General: (a) corrindex \~ PARTY + (age + birthplace + married) + TYPE OF POLITCIAN (b) income \~ PARTY + (age + birthplace + married) + TYPE OF POLITCIAN =\> as in (a) & (b) and with INTERACTION terms of PARTY with CONFOUNDERS

Prior need to analyse ALLSPEECHES for TYPE OF POLITICIAN Done (i) .number_speeches = How Vocal is politician Done (ii) .speechlength = avg. length of speech (iii) generality = how many basic terms per speech (iv) content = TOPIC MODELLING by various indizes -\> may need to transform numerical corrindex and income into categorical

Should numbers be dropped?

Birthplace: - number indicating the region a speaker was born in =\> How to deal with it? - Categorical value, as values from 1 to 23 have no meaning, ranking etc, except lower values indicate rather type A and higher values rather type B affiliation with region, BUT only more or less Done - construct var. for dominance of a party, where values all positive, no distinguishing between A & B, the higher the values the less dominated a region is by a party, thus the more intense potential campaigns Done - construct var. for political direction of a region, e.g. 3 mostly party A, -3 mostly party B speaker

# Install & Load Packages
```{r packages}
  #install.packages("tidyverse")
  #install.packages("tidytext")
  #install.packages("tidymodels")
  #install.packages("glmnet")
  #install.packages("glmnetUtils")
  #install.packages("wordcloud")

  library(tidyverse)
  library(tidytext)
  library(tidymodels)
  library(glmnet)
  library(glmnetUtils)
  library(wordcloud)
```

# Load Data
```{r LoadData}
  load("./Data/politicians.rdata")
```

# Mutate and Tidy data
```{r MutateTidy}
# Split column 'allspeeches' into tokens in column 'word', flattening the table into one-token-per-row
  data_unnested <- politicians %>%
    unnest_tokens(.word, allspeeches) %>%
    select(speaker, .word) %>%
    rename(.speaker = speaker)
  
  new_colnames <- paste0(".", colnames(politicians))
  colnames(politicians) <- new_colnames
  
  data <- politicians %>% 
    mutate(.corrupt = ifelse(.corrindex > 0, 1, 0), # Dummy whether more or less corrupt than the avg. politician
           .number_speeches = str_count(.allspeeches, "\\t House of Commons Hansard Debates for ") + 1, # create variable with number of speeches per speaker in variable 'allspeeches'
           .number_words = str_count(.allspeeches, " ") + 1, # Count total number of words
           .speechlength = .number_words/.number_speeches, # calc. avg. length of speech
           .partydummy = ifelse(.party == "B", 1, 0), # Transform to dummy for processing power
           .logincome = log(.income)) %>% # Transform income into log
    group_by(.birthplace) %>%
    # ASSUMPTION: all politicians within a birthplace are included in the data, or distribution and choice of entries are representative for a region
    mutate(.regionideology = sum(.party == 'A') - sum(.party == 'B'), # Identify Ideological tendence for each region
           .regionintensity = 1 - sum(.party == ifelse(sum(.party == 'A') > sum(.party == 'B'), 'A', 'B')) / n())  %>% # Identify the more prevalent party and calculate the ratio
    ungroup()
  
  # Transform Categorical variables to dummies, as transformation to as.factor() needs to much processing
  for (i in 1:21) {
      new_col <- paste0(".birthplace", i)
      data[new_col] <- ifelse(data$.birthplace==i, 1, 0)
    }
  
  apply(is.na(data), 2, which)
```

# Data Overview
Get quick overview of data - Summary of Data, Plotting main variables (corrindex, income, birthplace by Party) To-Do: Select output for paper and delete irrelevant memory things
```{r Overview}
summary(data)
  "
  - age between 35 and 70 with mean of 47.33
  - 23 birthplaces
  - corruption index between -2.633 and 3.292 with mean of 0
  - income between 52,231 and 532,325 with mean of 112,303
  "
  
  # Plot corrindex against income by Party association
  {
    plot_outcome <- ggplot(data = data, 
                           aes(x = .logincome, y = .corrindex, color = .party, group = .party)) +
      geom_point()
    print(plot_outcome)
    
    "
  Plot indicates that for both parties there is a positive correlation between
  corrupt behavior and income, thus more corrupt behavior indicates higher income.
  Slightly indicates that individuals associated to party B have less corrupt
  behavior
  "
  }
  
  # Plot Birthplace and Party Affiliation
  {
    plot_birthplace <- ggplot(data = data, aes(x = .party, fill = .party)) +
      geom_bar(position = "dodge") +
      facet_wrap(~.birthplace, scales = "free_x", ncol = 3) +
      labs(title = "Speakers by Party and Birthplace",
           x = "Party",
           y = "Number of Speakers") +
      theme_minimal()
    
    print(plot_birthplace)
  }
  
  # Density Plot of Income and Corruption and  by Party
  {
    plot_density_income <- ggplot(data = data,
                                  aes(x = .logincome, color = .party, group = .party)) +
      geom_density(linewidth = 1) +
      labs(x = "Income",
           y = "Density")
    print(plot_density_income)
    
    
    plot_density_corr <- ggplot(data = data,
                                aes(x = .corrindex, color = .party, group = .party)) +
      geom_density(linewidth = 1) +
      labs(x = "Corruption Index",
           y = "Density")
    print(plot_density_corr)
  }
```

# Sentiment
```{r sentiment}
# Calculate sentiment
{
  "
  when using dictionary 'bing no need to filter out the stem as it contains all
  word alterations
  "
  sentiment_dict <- get_sentiments("bing")
  sentiment <- data_unnested %>% 
    inner_join(sentiment_dict, relationship = "many-to-many", join_by(.word == word)) %>% # include only words that are part of the defined sentiment dictionary
    count(.speaker, sentiment) %>% # Counts for each speaker and both sentiments the number of words associated to each sentiment
    pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% #transforms table so each row is a speaker again
    # Mutate Command works only for 'bing' dictionary 
    mutate(.sentiment = positive - negative) %>% # calculate overall sentiment index
    rename(.positive = positive,
           .negative = negative)
  
  data <- data %>% left_join(sentiment, join_by(.speaker == .speaker))
}

# Plot sentiment on Log Income and Corruption Index
{
  plot_sentiment_inc <- ggplot(data = data,
                               aes(x=.sentiment, y = .logincome, color = .party, group = .party)) +
    geom_point() +
    geom_smooth(method='lm', formula=y~x, se=FALSE)
  plot_sentiment_inc
  
  plot_sentiment_corr <- ggplot(data = data,
                                aes(x=.sentiment, y = .corrindex, color = .party, group = .party)) +
    geom_point() +
    geom_smooth(method='lm', formula=y~x, se=FALSE)
  plot_sentiment_corr
}

rm(sentiment)
rm(sentiment_dict)
```

# Linear Estimation of constructed variables and Sentiment
```{r linestimation}
# Create string with names of independent variables, drop ID, dependent or double variables/derived variables
    independent_var <- colnames(data %>% 
                                  select(!c(".speaker", # ID variables
                                            ".corrindex", ".corrupt", ".income",".logincome", # Dependent
                                            ".party", ".allspeeches", ".birthplace", ".sentiment"))) # Derived variables
    
    # Create linear regression models for corruption index and income
    basemodel_corr <- lm((reformulate(independent_var, response =".corrindex")), data = data)
    basemodel_income <- lm((reformulate(independent_var, response =".logincome")), data = data)
    
    # Pritn out results of simple regression models
    summary(basemodel_corr)
    summary(basemodel_income)
```

# Speech Analysis
```{r speechanalysis}
  # Set a seed to reconstruct analyses
  set.seed(12345)
  # Define variables
  y_var <- '.corrindex' # Choose between '.corrindex' or '.logincome'
  word_type <- '.word' # Choose between '.word' or '.stem'
  
  
  # pick the words to keep as predictors
  # Need to choose between taking full words or word stems only (words = 16.6k vs. stem = 12k entries)
  {
    words_to_keep <- unique(data_unnested %>% # Drop double entries (drops words from 484k to 17k)
                              anti_join(get_stopwords(), join_by(.word  == word)) %>% # Drop words that are in dictionary stopwords, e.g.: I , me, my, myself...
                              mutate(.wordtype = if(word_type == '.word'){.word}
                                     else{wordStem(.word)}) %>%
                              count(.wordtype) %>%
                              
                              # is first filter necessary?
                              filter(str_detect(.wordtype, '.co|.com|.net|.edu|.gov|http', negate = TRUE)) |> # return all entries in 'word' that do NOT contain the listed words of URLs
                              filter(str_detect(.wordtype, '[0-9]', negate = TRUE)) |> # return all entries in 'words' that contain no numbers
                              
                              # How do I justify the value of 2?
                              filter(n > 2) |> # take only words that occur more than two times
                              pull(.wordtype)) # extract the column word as vector
  }
  
  # Construct Term Frequencies
  {
    tidy_speech <- data_unnested %>%
      transmute(.wordtype = if(word_type == '.word'){.word}
                else{wordStem(.word)}) %>%
      rename(.word = .wordtype) %>%
      filter(.word %in% words_to_keep) %>% # Take only words that are in previous created list
      count(.speaker, .word) %>% # count per speaker, identified through '.word' the number of a word occuring
      # Bind the term frequency and inverse document frequency of the data to the dataset
      bind_tf_idf(term = '.word', # Column containing terms as string or symbol
                  document = '.speaker', # Column containing document IDs as string or symbol
                  n = 'n') %>% # Column containing document-term counts as string or symbol
      select(.speaker, .word, tf) %>% # select ID, word and term-frequency column
      
      filter(.word != "") %>% # Filter out EMPTY strings
      # pivot wider into a document-term matrix
      pivot_wider(id_cols = '.speaker',
                  names_from = '.word',
                  values_from = 'tf',
                  values_fill = 0)
  }
  
  # Join Term Frequency with dataset
  {
    tidy_speech <- data %>%
      select(!c(".allspeeches")) %>%
      right_join(tidy_speech, by = '.speaker') %>%
      ungroup()
  }
  
  # Create train and test sample from data merge with Term Frequency
  # NEED TO JUSTIFY 08. - 0.2 division
  # Too little data for stratification by BIRTHPLACE
  {
    # split sample into training and test sample
    data_split <- initial_split(tidy_speech, prop = 0.8) # split into 80% training and 20% test
    
    train <- training(data_split)
    test <- testing(data_split)
  }
  
  # Remove dataset and clear memory for computation
  {
    rm(data_split)
    rm(data_unnested)
    rm(words_to_keep)
    gc()
  }
  
  # Analysis of Terms with various Models
  {
    # Define Variables
    Y_train <- train[y_var] # Dependent Variable
    W_train <- train[, !(colnames(train) %in% c(".speaker", ".corrindex", ".income", ".logincome", ".party", ".birthplace", ".partydummy", ".corrupt"))] # Covariates - Nuisance Parameters
    D_train <- train['.partydummy'] # Target Parameter
    X_train <- cbind(D_train, W_train) # Observed Regressors - 16,637 variables

    # Disselect variable for test
    Y_test <- test[y_var] # Dependent Variable
    W_test <- test[, !(colnames(test) %in% c(".speaker", ".corrindex", ".income", ".logincome", ".party", ".birthplace", ".partydummy", ".corrupt"))] # Covariates - Nuisance Parameters
    D_test <- test['.partydummy'] # Target Parameter
    X_test <- cbind(D_test, W_test) # Observed Regressors - 16,637 variables

    # (1) Simple Model
    
    # (2) Complex Model - Overfitting
    {
      # Linear Regression with all words
      model_comp_corr <- linear_reg () %>%  
        fit_xy(y=Y_train, x=X_train) # 75sec
      
      # Plot Fits
      {
        # Plot in-sample Fit
        plot_fit_comp_corr_in <- ggplot(data = (train %>% bind_cols(predict(model_comp_corr, X_train))),
                                        aes(x = .pred, y = .corrindex)) + 
          geom_point() +
          geom_abline(intercept = 0, slope = 1, size = 0.5)
        print(plot_fit_comp_corr_in)
        
        # Plot out-of-sample fit
        plot_fit_comp_corr_out <- ggplot(data = (test %>% bind_cols(predict(model_comp_corr, new_data=X_test))),
                                         aes(x = .pred, y = .corrindex)) +  
          geom_point() +
          geom_abline(intercept = 0, slope = 1, size = 0.5)
        print(plot_fit_comp_corr_out)
        }
      
      # Plot Words most strongly correlated
      {
        tidy(model_comp_corr)
      }
    }
    
    # (3) Regularized Model - LASSO - lambda 0.1
    {
      # Linear regression with LASSO penalty
      model_regul_corr <- linear_reg(penalty = 0.01, mixture = 1) %>%
        set_engine('glmnet') %>%
        fit_xy(y=Y_train, x=X_train)

      # Plot Fits
      {
        plot_fit_regul_corr_in <- ggplot(data = (train %>% bind_cols(predict(model_regul_corr, X_train))),
                                         aes(x = .pred, y = .corrindex)) + 
          geom_point() +
          geom_abline(intercept = 0, slope = 1, size = 0.5)
        print(plot_fit_regul_corr_in)
        
        # Plot out-of-sample fit
        plot_fit_regul_corr_out <- ggplot(data = (test %>% bind_cols(predict(model_regul_corr, X_test))),
                                          aes(x = .pred, y = .corrindex)) + 
          geom_point() +
          geom_abline(intercept = 0, slope = 1, size = 0.5)
        print(plot_fit_regul_corr_out)
        }
      
      # Plot most important words
      {
        tidy(model_regul_corr) %>%
          filter(estimate >= 1000) %>%
          ggplot(aes(x = estimate,
                     y = fct_reorder(term, estimate, .desc = TRUE))) +
          geom_col()
      }
      
    # (4) Regularized Model - LASSO - lambda Cross-validated(CV)
      {
        test_l <- cv.glmnet(x=as.matrix(X_train), y=as.matrix(Y_train), # Input
                            nfolds=10, # Number of folds to CV
                            type.measure = "mse") # the measure used to optimize CV
        plot(test_l) # first dotted line gives the minimum MSE from CV while second gives most regularized model
        log(test_l$lambda.min) # lambda with minimum MSE
        log(test_l$lambda.1se) # lambda of most regul. model such CV error is within 1 SE of the min.
        print(test_l) #Prints: Lambda, the index of which lambdas (position), the measure value to optimize CV, SE, number of nonzero coefficients
        
        # Extract non-zero coefficients at lambda.min
        non_zero_coefficients <- as.data.frame(as.matrix(coef(test_l, s = "lambda.min"))) %>%
          filter(.[[1]] != 0)
        print(non_zero_coefficients)
        
        test_first <- non_zero_coefficients[!rownames(non_zero_coefficients) %in% '(Intercept)' & 
                                       !grepl("^\\.", rownames(non_zero_coefficients)), ]
        names <- rownames(non_zero_coefficients)[!rownames(non_zero_coefficients) %in% '(Intercept)' & 
                                                      !grepl("^\\.", rownames(non_zero_coefficients))]
        
        test_wc <- wordcloud(words = names, freq=test_first)
      }

    }
  }
```
