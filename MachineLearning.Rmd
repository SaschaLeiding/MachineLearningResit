---
title: "MachineLearningResit"
author: '12393007'
date: "`r Sys.Date()`"
output: word_document
---

# Install & Load Packages
```{r packages}
  #install.packages("tidyverse")
  #install.packages("tidytext")
  #install.packages("tidymodels")
  #install.packages("glmnet")
  #install.packages("glmnetUtils")
  #install.packages("wordcloud")
  #install.packages("patchwork")
  #install.packages("hdm")
  #install.packages("grf")
  #install.packages("topicmodels")
  #install.packages("SnowballC")

  library(tidyverse)
  library(tidytext)
  library(tidymodels)
  library(glmnet)
  library(glmnetUtils)
  library(wordcloud)
  library(patchwork)
  library(hdm)
  library(grf)
  library(topicmodels)
  library(SnowballC)

```

# Load Data
```{r LoadData}
  load("./Data/politicians.rdata")
```

# Mutate and Tidy data
```{r MutateTidy}
# Split column 'allspeeches' into tokens in column 'word', flattening the table into one-token-per-row
  data_unnested <- politicians %>%
    unnest_tokens(.word, allspeeches) %>%
    select(speaker, .word) %>%
    rename(.speaker = speaker)
  
  new_colnames <- paste0(".", colnames(politicians))
  colnames(politicians) <- new_colnames
  
  data <- politicians %>% 
    mutate(.corrupt = ifelse(.corrindex > 0, 1, 0), # Dummy whether more or less corrupt than the avg. politician
           .number_speeches = str_count(.allspeeches, "\\t House of Commons Hansard Debates for ") + 1, # create variable with number of speeches per speaker in variable 'allspeeches'
           .number_words = str_count(.allspeeches, " ") + 1, # Count total number of words
           #.speechlength = .number_words/.number_speeches, # calc. avg. length of speech
           .partydummy = ifelse(.party == "B", 1, 0), # Transform to dummy for processing power
           .logincome = log(.income)) %>% # Transform income into log
    group_by(.birthplace) %>%
    # ASSUMPTION: all politicians within a birthplace are included in the data, or distribution and choice of entries are representative for a region
    mutate(.regionideology = sum(.party == 'A') - sum(.party == 'B'), # Identify Ideological tendency for each region
           .regionintensity = 1 - sum(.party == ifelse(sum(.party == 'A') > sum(.party == 'B'), 'A', 'B')) / n())  %>% # Identify the more prevalent party and calculate the ratio
    ungroup()
  
  # Transform Categorical variables to dummies, as transformation to as.factor() needs to much processing
  for (i in 1:21) {
      new_col <- paste0(".birthplace", i)
      data[new_col] <- ifelse(data$.birthplace==i, 1, 0)
    }
  
  apply(is.na(data), 2, which)
  
  rm(i)
  rm(new_col)
  rm(new_colnames)
```

# Data Overview
Get quick overview of data - Summary of Data, Plotting main variables (corrindex, income, birthplace by Party) To-Do: Select output for paper and delete irrelevant memory things
```{r Overview}
summary(data)
  "
  - age between 35 and 70 with mean of 47.33
  - 23 birthplaces
  - corruption index between -2.633 and 3.292 with mean of 0
  - income between 52,231 and 532,325 with mean of 112,303
  "
  
  # Plot corrindex against income by Party association
  {
    plot_outcome <- ggplot(data = data, 
                           aes(x = .logincome, y = .corrindex, color = .party, group = .party)) +
      geom_point()
    print(plot_outcome)
    
    "
  Plot indicates that for both parties there is a positive correlation between
  corrupt behavior and income, thus more corrupt behavior indicates higher income.
  Slightly indicates that individuals associated to party B have less corrupt
  behavior
  "
  }
  
  # Plot Birthplace and Party Affiliation
  {
    plot_birthplace <- ggplot(data = data, aes(x = .party, fill = .party)) +
      geom_bar(position = "dodge") +
      facet_wrap(~.birthplace, scales = "free_x", ncol = 3) +
      labs(title = "Speakers by Party and Birthplace",
           x = "Party",
           y = "Number of Speakers") +
      theme_minimal()
    
    print(plot_birthplace)
  }
  
  # Density Plot of Income and Corruption and  by Party
  {
    plot_density_income <- ggplot(data = data,
                                  aes(x = .logincome, color = .party, group = .party)) +
      geom_density(linewidth = 1) +
      labs(x = "Income",
           y = "Density")
    print(plot_density_income)
    
    
    plot_density_corr <- ggplot(data = data,
                                aes(x = .corrindex, color = .party, group = .party)) +
      geom_density(linewidth = 1) +
      labs(x = "Corruption Index",
           y = "Density")
    print(plot_density_corr)
  }
```

# Sentiment
```{r sentiment}
# Calculate sentiment
{
  "
  when using dictionary 'bing no need to filter out the stem as it contains all
  word alterations
  "
  sentiment_dict <- get_sentiments("bing")
  sentiment <- data_unnested %>% 
    inner_join(sentiment_dict, relationship = "many-to-many", join_by(.word == word)) %>% # include only words that are part of the defined sentiment dictionary
    count(.speaker, sentiment) %>% # Counts for each speaker and both sentiments the number of words associated to each sentiment
    pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% #transforms table so each row is a speaker again
    # Mutate Command works only for 'bing' dictionary 
    mutate(.sentiment = positive - negative) %>% # calculate overall sentiment index
    rename(.positive = positive,
           .negative = negative)
  
  data <- data %>% left_join(sentiment, join_by(.speaker == .speaker))
}

# Plot sentiment on Log Income and Corruption Index
{
  plot_sentiment_inc <- ggplot(data = data,
                               aes(x=.sentiment, y = .logincome, color = .party, group = .party)) +
    geom_point() +
    geom_smooth(method='lm', formula=y~x, se=FALSE)
  print(plot_sentiment_inc)
  
  plot_sentiment_corr <- ggplot(data = data,
                                aes(x=.sentiment, y = .corrindex, color = .party, group = .party)) +
    geom_point() +
    geom_smooth(method='lm', formula=y~x, se=FALSE)
  print(plot_sentiment_corr)
}

rm(sentiment)
rm(sentiment_dict)
```

# Linear Estimation of constructed variables and Sentiment
```{r linestimation}
# Create string with names of independent variables, drop ID, dependent or double variables/derived variables
    independent_var <- colnames(data %>% 
                                  select(!c(".speaker", # ID variables
                                            ".corrindex", ".corrupt", ".income",".logincome", # Dependent
                                            ".party", ".allspeeches", ".birthplace", ".sentiment"))) # Derived variables
    
    # Create linear regression models for corruption index and income
    basemodel_corr <- lm((reformulate(independent_var, response =".corrindex")), data = data)
    basemodel_income <- lm((reformulate(independent_var, response =".logincome")), data = data)
    
    # Pritn out results of simple regression models
    summary(basemodel_corr) # If party=1, thus B, .corrindex drops by 0.1989 at 0.001% significance level
    summary(basemodel_income)
```

# Prepare Speech Analysis
```{r prepspeechanalysis}
  # Set a seed to reconstruct analyses
  set.seed(12345)
  # Define variables
  y_var <- '.logincome' # Choose between '.corrindex' or '.logincome'
  word_type <- '.word' # Choose between '.word' or '.stem'

  # pick the words to keep as predictors
  # Choose between taking full words or word stems only (words = 16.6k vs. stem = 12k entries)
  {
    words_to_keep <- unique(data_unnested %>% # Drop double entries (drops words from 484k to 16,603) with smart to 16,280 and .stem dropping to 12,030
                              anti_join(get_stopwords(source="smart"), # standard=175 words vs, smart 571 words
                                        join_by(.word  == word)) %>% # Drop words that are in dictionary stopwords, e.g.: I , me, my, myself...
                              mutate(.wordtype = if(word_type == '.word'){.word}
                                     else{wordStem(.word)}) %>%
                              count(.wordtype) %>%
                              filter(str_detect(.wordtype, '.co|.com|.net|.edu|.gov|http', negate = TRUE)) |> # return all entries in 'word' that do NOT contain the listed words of URLs
                              filter(str_detect(.wordtype, '[0-9]', negate = TRUE)) |> # return all entries in 'words' that contain no numbers
                              filter(n > 2) |> # take only words that occur more than two times
                              pull(.wordtype)) # extract the column word as vector
  }
  
  # Construct Term Frequencies
  {
    tidy_speech <- data_unnested %>%
      transmute(.wordtype = if(word_type == '.word'){.word}
                else{wordStem(.word)}) %>%
      rename(.word = .wordtype) %>%
      filter(.word %in% words_to_keep) %>% # Take only words that are in previous created list
      count(.speaker, .word) %>% # count per speaker, identified through '.word' the number of a word occuring
      # Bind the term frequency and inverse document frequency of the data to the dataset
      bind_tf_idf(term = '.word', # Column containing terms as string or symbol
                  document = '.speaker', # Column containing document IDs as string or symbol
                  n = 'n') %>% # Column containing document-term counts as string or symbol
      select(.speaker, .word, tf) %>% # select ID, word and term-frequency column
      
      filter(.word != "") %>% # Filter out EMPTY strings
      # pivot wider into a document-term matrix
      pivot_wider(id_cols = '.speaker',
                  names_from = '.word',
                  values_from = 'tf',
                  values_fill = 0)
  }
  
  # Join Term Frequency with dataset
  {
    tidy_speech <- data %>%
      select(!c(".allspeeches")) %>%
      right_join(tidy_speech, by = '.speaker') %>%
      ungroup()
  }
  
  # Create train and test sample from data merge with Term Frequency
  # Too little data for stratification by BIRTHPLACE
  {
    # split sample into training and test sample
    data_split <- initial_split(tidy_speech, prop = 0.8) # split into 80% training and 20% test
    
    train <- training(data_split)
    test <- testing(data_split)
  }
  
  # Remove dataset and clear memory for computation
  {
    rm(data_split)
    gc()
  }
```

# Topic Modeling
Derived topics can be used as proxies for overall speeches, requires less computing power, and instead of using all tokens does not quickly lead to overfitting, without using Regularization methods in standard causal estimation
```{r topicmodel}
#save(topic_model, file = "topic_model.RData")
#save(topic_model_22, file = "topic_model_22.RData")

load("./topic_model.RData")#  contains topic model with 10 topics
#load("./topic_model_22.RData") # contains topic model with 22 topics

# Some standard words are in every parliamentary speech and not filtered out by 'stopwords'
special_list <- c("hon", "government", "people", "mr", "mrs", "one", "can", 
                  "also","member", "members", "house", "commitee")

# Transform data in readable format for 'LDA'
speech_dtm <- data_unnested %>%
  transmute(.wordtype = if(word_type == '.word'){.word}
            else{wordStem(.word)}) %>%
  rename(.word = .wordtype) %>%
  filter(.word %in% words_to_keep) %>% # Take only words that are in previous created list
  count(.speaker, .word) %>%
  filter(n > 2 & nchar(.word) > 2 & !(.word %in% special_list)) %>% # words that occurred more than two times
  cast_dtm(.speaker, .word, n)

# Run LDA
#topic_model <- LDA(speech_dtm, k=10, control=list(seed = 12345))

# obtain the topic-specific probability vectors theta_1,...,theta_K
# (in the LDA output, these are denoted by "beta")
speech_topics <- tidy(topic_model, matrix = "beta")

# find top 10 terms in each topic
speech_top_terms <- speech_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

# plot topic-specific probabilities of the top words in each topic
speech_top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

# Derive
topicsprob10 <- as.data.frame(posterior(topic_model)$topics)# Extract topic probabilities
colnames(topicsprob10) <- paste0(".topic", 1:ncol(topicsprob10)) # Rename columns
tidy_speech <- cbind(tidy_speech, topicsprob10)

rm(data_unnested)
rm(words_to_keep)
``` 

# 2 Conduct Speech Analysis
## Split Data and Define Variables
```{r splitdata}
# Set a seed to reconstruct analyses
set.seed(12345)
#y_var <- '.corrindex' # Choose between '.corrindex' or '.logincome'
# y_var must be same in code-chunk 'prepspeechanalysis'

# Create train and test sample
# Too little data for stratification by BIRTHPLACE
{
  # split sample into training and test sample
  data_split <- initial_split(tidy_speech, prop = 0.8) # split into 80% training and 20% test
    
  train <- training(data_split)
  test <- testing(data_split)
}

# Define Variables
{
  confounder_names <- c(".speaker", # Identifier
                        ".corrindex", ".corrupt", ".income", ".logincome", # Outcome Variables
                        ".party", ".partydummy", # Target Parameter
                        grep("\\.birthplace", colnames(train), value =TRUE)) # Instrument
  # Train Data
  Y_train <- train[y_var] # Outcome Variable
  W_train <- train[, !(colnames(train) %in% confounder_names)]# Confounders - Nuisance Parameters
  D_train <- train['.partydummy'] # Target Parameter
  Z_train <- train %>% select(starts_with(".birthplace") & !(".birthplace")) # Instrument
  X_train <- cbind(D_train, Z_train, W_train) # Observed Regressors - 16,635 variables

  # Test Data
  Y_test <- test[y_var] # Outcome Variable
  W_test <- test[, !(colnames(test) %in% confounder_names)] # Covariates - Nuisance Parameters
  D_test <- test['.partydummy'] # Target Parameter
  Z_test <- test %>% select(starts_with(".birthplace") & !(".birthplace")) # Instrument
  X_test <- cbind(D_test, Z_test, W_test) # Observed Regressors - 16,635 variables
  
  # Full Data
  Y_full <- tidy_speech[y_var] # Outcome Variable
  W_full <- tidy_speech[, !(colnames(tidy_speech) %in% confounder_names)] # Covariates - Nuisance Parameters
  D_full <- tidy_speech['.partydummy'] # Target Parameter
  Z_full <- tidy_speech %>% select(starts_with(".birthplace") & !(".birthplace")) # Instrument
  X_full <- cbind(D_full, Z_full, W_full) # Observed Regressors - 16,635 variables
  
  # Other
  simplemodels_list <- list() # List containing models
  in_sample_plots <- list() # List for in-sample fits of models
  out_sample_plots <- list() # List for out-of-sample fits of models
  name_outcome <- ifelse(y_var == ".corrindex", "corr", "inc")
  words_hetero <- c() # vector containing non-zero coefficients
}
```

## 2.1 Variable Selection
```{r variableSelection}
# Set a seed to reconstruct analyses
set.seed(12345)

# Train Models
# (1) OLS, (2) LASSO(lambda=0.1), (3) LASSO(lambda=CV), (4)Double LASSO
{
  # (1) Simple Model - Linear Regression with all words
  simplemodels_list[[paste0("model_comp_", name_outcome)]]<- linear_reg() %>%
    fit_xy(y = Y_train, x = X_train)

  # (2) Regularized Model - LASSO
  simplemodels_list[[paste0("model_regul_", name_outcome)]]<- linear_reg(penalty = 0.01, # Amount of Regularization (lambda)
                                                                         mixture = 1) %>% # Pure LASSO model
    set_engine('glmnet') %>%
    fit_xy(y = Y_train, x = X_train)

  # (3) Regularized Model - LASSO - lambda Cross-validated (CV)
  simplemodels_list[[paste0("model_cvregul_", name_outcome)]]<- cv.glmnet(x = as.matrix(X_train), 
                                                    y = as.matrix(Y_train),
                                                    nfolds = 10, 
                                                    type.measure = "mse")
  
  # (4) Double LASSO - CV
  {
    # Partial out W (confounders) from Y (outcome)
    fit.lasso.Y <- cv.glmnet(x=as.matrix(W_train), y=as.matrix(Y_train))
    fitted.lasso.Y <- predict(fit.lasso.Y, newx=as.matrix(W_train), s="lambda.min")
    Ytilde_corr <- as.matrix(Y_train) - fitted.lasso.Y
    
    # Partial out w(confounders) from D(Party Affiliation)
    fit.lasso.D <- cv.glmnet(x=as.matrix(W_train), y=as.matrix(D_train))
    fitted.lasso.D <- predict(fit.lasso.D, newx=as.matrix(W_train), s="lambda.min")
    Dtilde_corr <- as.matrix(D_train) - fitted.lasso.D
    
    # Run OLS of Ytilde on Dtilde
    fit.doubleLASSO <- lm(Ytilde_corr ~ Dtilde_corr)
    beta1hat <- coef(fit.doubleLASSO)[2]
    print(beta1hat)
  }
}

# Plotting of in-/out-of-sample fit for models (1)-(4) with test and train data
{
  # Loop through each model in simplemodels_list
  for (i in seq_along(simplemodels_list)) {
    model <- simplemodels_list[[i]]
    model_name <- names(simplemodels_list)[i]

    # Check if the model is a cv.glmnet model
    if (inherits(model, "cv.glmnet")) {
      # For cv.glmnet, use lambda.min
      predict_in_sample <- predict(model, newx = as.matrix(X_train), s = "lambda.min", type = "link")
      predict_out_sample <- predict(model, newx = as.matrix(X_test), s = "lambda.min", type = "link")

      # Plotting for cv.glmnet
      plot_in_sample <- ggplot(data = (train %>% bind_cols(predict_in_sample)),
                               aes(x = lambda.min, y = get(y_var), color = .party, group = .party)) +
        geom_point() +
        geom_abline(intercept = 0, slope = 1, size = 0.5) +
        labs(x = paste("Predicted", y_var), y = paste("Actual", y_var)) +
        theme(legend.position="none")
      
      plot_out_sample <- ggplot(data = (test %>% bind_cols(predict_out_sample)),
                              aes(x = lambda.min, y = get(y_var), color = .party, group = .party)) +  
        geom_point() +
        geom_abline(intercept = 0, slope = 1, size = 0.5) +
        labs(x = paste("Predicted", y_var), y = paste("Actual", y_var))+
        theme(legend.position="none")
      } else {
        # For other models
        predict_in_sample <- predict(model, X_train)
        predict_out_sample <- predict(model, new_data = X_test)
        
        # Common code for plotting fits
        plot_in_sample <- ggplot(data = (train %>% bind_cols(predict_in_sample)),
                                 aes(x = .pred, y = get(y_var), color = .party, group = .party)) +
          geom_point() +
          geom_abline(intercept = 0, slope = 1, size = 0.5) +
          labs(x = paste("Predicted", y_var), y = paste("Actual", y_var)) +
          theme(legend.position="none")
        
        plot_out_sample <- ggplot(data = (test %>% bind_cols(predict_out_sample)),
                                  aes(x = .pred, y = get(y_var), color = .party, group = .party)) +
          geom_point() +
          geom_abline(intercept = 0, slope = 1, size = 0.5) +
          labs(x = paste("Predicted", y_var), y = paste("Actual", y_var)) +
          theme(legend.position="none")
        }
    
    # Store combined in-sample and out-of-sample plots
    in_sample_plots[[model_name]] <- plot_in_sample
    out_sample_plots[[model_name]] <- plot_out_sample
    }
  
  # Special Case: Double LASSO as rlassoEffects takes too long with that many variables
  {
    # Step 1: calculate out-of sample fit
    {
      # Use the fitted models from training to predict on test set
      fitted.lasso.Y.test <- predict(fit.lasso.Y, newx = as.matrix(W_test), s = "lambda.min")
      Ytilde_test <- as.matrix(Y_test) - fitted.lasso.Y.test
      
      fitted.lasso.D.test <- predict(fit.lasso.D, newx = as.matrix(W_test), s = "lambda.min")
      Dtilde_test <- as.matrix(D_test) - fitted.lasso.D.test
      
      # OLS of Ytilde on Dtilde for test set
      fit.doubleLASSO.test <- lm(Ytilde_test ~ Dtilde_test)
    }
    
    # Step 2: Plot in/-out of sample Fits
    {
      # Adding the fitted values from the Double LASSO to the train and test datasets
      train$DLpred_logincome <- fitted.lasso.Y + fit.doubleLASSO$fitted.values
      test$DLpred_logincome <- fitted.lasso.Y.test + fit.doubleLASSO.test$fitted.values
      
      # Plotting for Double LASSO - In-sample (training set)
      plot_in_sample <- ggplot(train, 
                           aes(x = DLpred_logincome,
                               y = get(y_var), color = .party, group = .party)) +
        geom_point() +
      geom_abline(intercept = 0, slope = 1, size = 0.5) +
      labs(x = paste("Predicted", y_var), y = paste("Actual", y_var)) +
      theme(legend.position="none")
  
    # Plotting for Double LASSO - Out-of-sample (test set)
    plot_out_sample <- ggplot(test, 
                            aes(x = DLpred_logincome,
                                y = get(y_var), color = .party, group = .party)) +
      geom_point() +
      geom_abline(intercept = 0, slope = 1, size = 0.5) +
      labs(x = paste("Predicted", y_var), y = paste("Actual", y_var)) +
      theme(legend.position="none")
    }
    
    # Step 3: Save plots in list
    # Store combined in-sample and out-of-sample plots
    in_sample_plots[[paste0("model_DLregul_", name_outcome)]] <- plot_in_sample
    out_sample_plots[[paste0("model_DLregul_", name_outcome)]] <- plot_out_sample
  }
  
  # Combine all Plots
  {
    # Create labels for the plots
    label_plots <- map(c("OLS", "LASSO", "LASSO CV", "Double LASSO"), 
                       ~ ggplot() +
                         theme_void() +
                         theme(plot.margin = margin(0, 0, 0, 0)) +
                         annotate("text", x = 0.5, y = 0.5, label = .x, fontface = "bold", hjust = 0.5))
    
    # Combine everything
    final_plot <- (Reduce('+', label_plots) + plot_layout(ncol = 4)) / 
      (Reduce('+', in_sample_plots) + plot_layout(ncol = 4)) / 
      (Reduce('+', out_sample_plots) + plot_layout(ncol = 4)) +
      plot_layout(heights = c(1, 4, 4))
  }

# Print the final combined plot
print(final_plot)
}

  
# Check Propensity Scores on full dataset
{
  # (4) Regression forest for Propensity
  ZW_full <- cbind(Z_full, W_full)
  propensity.forest <- regression_forest(X= ZW_full,
                                         Y= as.matrix(D_full))
  
  # Predict probability of treatment assignment for each value:
  propensity.hat <- predict(propensity.forest)
  propensity.importance <- variable_importance(propensity.forest)
  propensity.importance.cols <- order(propensity.importance, decreasing = TRUE)
  colnames(ZW_full)[propensity.importance.cols][1:24] # Print out 10 most imp. variables
  
  plot_hist_propensity <- hist(propensity.hat[, "predictions"], 
                               main = "Estimate of propensity score distribution", 
                               xlab = "Propensity score")
  ZW_full$p.hat <- propensity.hat$predictions
}

# Extract non-zero coefficients from Regularizing models - retrieve in total 354 words
{
  # (2) LASSO - lambda = 0.1 - retrieves 923 variables - NOT APPLIED see FITS
  {
  #  words_hetero <- append(words_hetero,
  #                       tidy(simplemodels_list[[paste0("model_regul_", name_outcome)]]) %>%
  #                         filter(estimate != 0 & !grepl("^\\.", term)) %>% 
  #                         pull(term))
  }
    
  # (3) LASSO - lambda = CV - retrieves 139 variables
  words_hetero <- append(words_hetero,
                          as.data.frame(
                            as.matrix(
                              coef(simplemodels_list[[paste0("model_cvregul_", 
                                                             name_outcome)]],
                                   s = "lambda.min"))) %>%
                           tibble::rownames_to_column(var = "term") %>%
                           filter(s1 != 0 & !grepl("^\\.", term)) %>%
                           pull(term))

  # (4) Double LASSO --- fit.lasso.D; fit.lasso.Y 436 variables
  words_hetero <- append(words_hetero,
                          as.data.frame(as.matrix(coef(fit.lasso.D, s = "lambda.min"))) %>%
                           tibble::rownames_to_column(var = "term") %>%
                           filter(s1 != 0 & !grepl("^\\.", term)) %>%
                           pull(term))
  words_hetero <- append(words_hetero,
                         as.data.frame(as.matrix(coef(fit.lasso.Y, s = "lambda.min"))) %>%
                           tibble::rownames_to_column(var = "term") %>%
                          filter(s1 != 0 & !grepl("^\\.", term)) %>%
                          pull(term))
  
  words_hetero <- setdiff(unique(words_hetero), "(Intercept)")
}

# Remove unneccesary Data
{
  
}
```

## 2.2 Estimate Party Affiliation effect on Outcome Variables
```{r MLestimation}
# Filter data on relevant words only
{
  W_selec <- W_full %>% select(starts_with("."), all_of(words_hetero))
}
# Double LASSO with all units of observation but only selected words
{
  # Partial out W (confounders) from Y (outcome)
  fit.lasso.Y.selec <- cv.glmnet(x=as.matrix(W_selec), y=as.matrix(Y_full))
  fitted.lasso.Y.selec <- predict(fit.lasso.Y.selec, newx=as.matrix(W_selec), s="lambda.min")
  Ytilde.selec <- as.matrix(Y_full) - fitted.lasso.Y.selec
    
  # Partial out w(confounders) from D(Party Affiliation)
  fit.lasso.D.selec <- cv.glmnet(x=as.matrix(W_selec), y=as.matrix(D_full))
  fitted.lasso.D.selec <- predict(fit.lasso.D.selec, newx=as.matrix(W_selec), s="lambda.min")
  Dtilde.selec<- as.matrix(D_full) - fitted.lasso.D.selec
    
  # Run OLS of Ytilde on Dtilde
  fit.doubleLASSO.selec <- lm(Ytilde.selec ~ Dtilde.selec)
  beta1hat.selec <- coef(fit.doubleLASSO.selec)[2]
  print(beta1hat.selec)
}

# Causal Forest with all units of observation but only selected words
{
  fit.forest.selec <- causal_forest(as.matrix(W_selec), as.matrix(Y_full), 
                                 as.matrix(D_full), tune.parameters = "all")
  
  # Calculate the ATE:
  average_treatment_effect(fit.forest.selec)
    
  # The function best_linear_projection returns estimates of the linear relation between
  # the (conditional) ATE and the covariates. While we have no reason to assume that
  # the relation is linear, it presents a good first step to investigate if heterogeneity
  # is present and which variables appear to drive it.
  blp.selec <- best_linear_projection(fit.forest.selec, as.matrix(W_selec))
  blp.selec
}
```

## 2.3 Explore Heterogeneity
```{r heterogeneityestimation}
# Create interaction term - TO BE ADAPTED
{
  # Create  Interaction terms of .partydummy with all previously identified covariates
  D_interact <- cbind(D_full, W_selec) %>%
    mutate(across(everything(), ~ . * .partydummy, .names = ".partydummyx{.col}")) %>%
    select(where(~ var(.) != 0) & starts_with(".party")) %>%
    select(!.partydummyx.partydummy)
  X_interact <- cbind(D_interact, W_selec)
}

Y_full_inc <- tidy_speech[".corrindex"] # Outcome Variable
index_interact <- grep("^\\.party", colnames(X_interact))

# Double LASSO
# OLS estimation of residuals  not inherently robust to treatment effect heterogeneity
{
  fit.doubleLASSO.inter.corr <- rlassoEffects(x=X_interact,
                                              y=Y_full,
                                              index=index_interact,
                                              method='partialling out')
  fit.doubleLASSO.inter.inc <- rlassoEffects(x=X_interact,
                                              y=Y_full_inc,
                                              index=index_interact,
                                              method='partialling out')
  
  save(fit.doubleLASSO.inter.corr, file = "fit.doubleLASSO.inter.corr.RData")
  save(fit.doubleLASSO.inter.inc, file = "fit.doubleLASSO.inter.inc.RData")
}

# Show CATE by birthplace & Age0 for both outcome variables
```


# 3 Instrumental Variable
```{r Instrumental Variable}
# Test Relevance condition
{
  # Normal IV (OLS does not work because too many variables due to all words contained), hence no residuals
  predictor_names <- gsub("[^[:alnum:] ]", "", c(colnames(Z_full), colnames(W_selec)))
  formula.fs <- reformulate(predictor_names, response = "partydummy")
  ZW <- cbind(Z_full, W_selec)
  colnames(ZW) <- predictor_names
  
  first_stage_model <- lm(formula.fs,
                          data = (cbind((D_full %>% rename("partydummy" = ".partydummy")), ZW)))
  first_stage_fstat <- summary(first_stage_model)$fstatistic
  
  # Calculate the F-statistic value
  fstat_value <- (first_stage_fstat[1] / first_stage_fstat[2])
  print(paste("F-statistic:", fstat_value))
}

#Quadruple LASSO IV - because instrument is multinomial
{
  set.seed(12345)
  # 1. Use LASSO to predict D with Z AND W
  fit.lasso.Dpred <- cv.glmnet(y=as.matrix(D_full), as.matrix((cbind(Z_full, W_full))))
  fitted.lasso.Dpred <- predict(fit.lasso.Dpred, newx=as.matrix((cbind(Z_full, W_full))), s="lambda.min")
  
  # 2. Use LASSO to partial out W
  # (a) partial out W from Y - done above
  fit.lasso.Y <- cv.glmnet(as.matrix(W_full), as.matrix(Y_full))
  fitted.lasso.Y <- predict(fit.lasso.Y, newx=as.matrix(W_full), s="lambda.min")
  Ytilde <- as.matrix(Y_full) - fitted.lasso.Y
  
  # (b) partial out W from D - done above
  fit.lasso.D <- cv.glmnet(as.matrix(W_full), as.matrix(D_full))
  fitted.lasso.D <- predict(fit.lasso.D, newx=as.matrix(W_full), s="lambda.min")
  Dtilde <- as.matrix(D_full) - fitted.lasso.D
  
  # (c) Partial out W from D_pred from 1.
  fit.lasso.QDpred <- cv.glmnet(as.matrix(W_full), fitted.lasso.Dpred)
  fitted.lasso.QDpred <- predict(fit.lasso.QDpred, newx=as.matrix(W_full), s="lambda.min")
  QDpredtilde <- fitted.lasso.Dpred - fitted.lasso.QDpred
  
  # 3. Estimate IV coefficient
  fit.QDLASSO.rf <- lm(Ytilde ~ QDpredtilde)
  coef.QDLASSO.rf <- coef(fit.QDLASSO.rf)[2]
  fit.QDLASSO.fs <- lm(Dtilde~ QDpredtilde)
  coef.QDLASSO.fs <- coef(fit.QDLASSO.fs)[2]		# first-stage coefficient
  
  beta1hat.QDLASSO <- coef.QDLASSO.rf / coef.QDLASSO.fs	# IV coefficient
  beta1hat.QDLASSO

  
}
```


